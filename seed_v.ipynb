{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import required packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Global seed set to 42\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cuda:0\n"
     ]
    }
   ],
   "source": [
    "## Standard libraries\n",
    "import os\n",
    "import numpy as np\n",
    "import random\n",
    "import json\n",
    "from PIL import Image\n",
    "from collections import defaultdict\n",
    "from statistics import mean, stdev\n",
    "from copy import deepcopy\n",
    "import pickle\n",
    "import re\n",
    "\n",
    "## Imports for plotting\n",
    "import matplotlib.pyplot as plt\n",
    "plt.set_cmap('cividis')\n",
    "%matplotlib inline\n",
    "from IPython.display import set_matplotlib_formats\n",
    "import matplotlib\n",
    "matplotlib.rcParams['lines.linewidth'] = 2.0\n",
    "import seaborn as sns\n",
    "sns.reset_orig()\n",
    "\n",
    "## tqdm for loading bars\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "## PyTorch\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.utils.data as data\n",
    "import torch.optim as optim\n",
    "\n",
    "## Torchvision\n",
    "import torchvision\n",
    "from torchvision.datasets import CIFAR100, SVHN\n",
    "from torchvision import transforms\n",
    "\n",
    "# PyTorch Lightning\n",
    "import pytorch_lightning as pl\n",
    "from pytorch_lightning.callbacks import LearningRateMonitor, ModelCheckpoint\n",
    "pl.seed_everything(42)\n",
    "\n",
    "# Import tensorboard\n",
    "%load_ext tensorboard\n",
    "\n",
    "device = torch.device(\"cuda:0\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "print(\"Device:\", device)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Set folder paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Path to the folder where the datasets are/should be downloaded (e.g. CIFAR10)\n",
    "DATASET_PATH = \"raw_data/seed-v/merged_data\"\n",
    "# Path to the folder where the pretrained models are saved\n",
    "CHECKPOINT_PATH = \"saved_models/seed-v\""
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define dataset class for SEEDV and initialize a new dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SEEDV(data.Dataset):\n",
    "    def __init__(self, emotion_dict, num_participants, data_dir):\n",
    "        self.emotion_dict = emotion_dict\n",
    "        self.num_participants = num_participants\n",
    "        self.data_dir = data_dir\n",
    "        self.tensor_dataset = self.load_data()\n",
    "    \n",
    "    def load_data(self):\n",
    "        dataset = None           \n",
    "        for file in os.listdir(self.data_dir):\n",
    "            file_path = os.path.join(self.data_dir, file)\n",
    "            if dataset is None:\n",
    "                target = np.int64(re.findall(\"\\d+\", file)[0])\n",
    "                for i in range(1, len(np.load(file_path))):\n",
    "                    target = np.hstack((target, int(re.findall(\"\\d+\", file)[0])))\n",
    "                dataset = np.load(file_path)\n",
    "            else:\n",
    "                for i in range(len(np.load(file_path))):\n",
    "                    target = np.hstack((target, np.int64(re.findall(\"\\d+\", file)[0])))\n",
    "                dataset = np.vstack((dataset, np.load(file_path)))\n",
    "\n",
    "        tensor_dataset = data.TensorDataset(torch.from_numpy(dataset[:, :-1]), torch.from_numpy(dataset[:, -1]), torch.from_numpy(target))\n",
    "                    \n",
    "        return tensor_dataset\n",
    "    \n",
    "    def get_all_data(self):\n",
    "        \"\"\"\n",
    "        same as self.__getitem__(self, idx) but instead of a specific index\n",
    "        this function will return all the tuple that contains all features and \n",
    "        combined labels \n",
    "        \"\"\"\n",
    "        all_features = None\n",
    "        all_combined_labels = None\n",
    "\n",
    "        for idx in range(len(self.tensor_dataset)):\n",
    "            if all_features == None:\n",
    "                all_features, all_combined_labels = self.__getitem__(idx)\n",
    "            else:\n",
    "                features, combined_labels = self.__getitem__(idx)\n",
    "                all_features = torch.vstack((all_features, features))\n",
    "                all_combined_labels = torch.vstack((all_combined_labels, combined_labels))\n",
    "        \n",
    "        return all_features, all_combined_labels\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.landmarks_frame)\n",
    "        \n",
    "    def __getitem__(self, idx):\n",
    "        \"\"\"\n",
    "        return a tuple of features, combined_label (from participant and emotion)\n",
    "        p1: 0 1 2 3 4,\n",
    "        p2: 5 6 7 8 9,\n",
    "        ...\n",
    "        p16: 75 76 77 78 79\n",
    "\n",
    "        we are only given participant # and emotion #\n",
    "        p1_0: 0 = (1 - 1) * 5\n",
    "        p1_1: 1 = 0 + 1\n",
    "        p1_2: 2 = 0 + 2\n",
    "        p1_3: 3 = 0 + 3\n",
    "        p1_4: 4 = 0 + 4\n",
    "        p2_0: 5 = (2 - 1) * 5\n",
    "        p2_1: 6 = 5 + 1\n",
    "        p2_2: 7 = 5 + 2\n",
    "        p2_3: 8 = 5 + 3\n",
    "        p2_4: 9 = 5 + 4\n",
    "        ..\n",
    "        p16_0: 75 = (16 - 1) * 5 = 75\n",
    "        ...\n",
    "        \"\"\"\n",
    "        features = self.tensor_dataset[idx][0]\n",
    "        emotion_num = self.tensor_dataset[idx][1]\n",
    "        participant_num = self.tensor_dataset[idx][2]\n",
    "        base = (participant_num - 1) * len(self.emotion_dict)\n",
    "        combined_label = base + emotion_num\n",
    "\n",
    "        return features, combined_label.to(torch.int64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed_v = SEEDV(emotion_dict = {0: 'disgust', 1: 'fear', 2: 'sad', 3: 'neutral', 4: 'happy'}, num_participants=16, data_dir=DATASET_PATH)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Perform a train-val-test split by label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "classes = torch.randperm(5*16) # Generate random permutation of numbers from 0 to 79\n",
    "train_classes, val_classes, test_classes = classes[:64], classes[64:72], classes[72:] # 80-10-10 split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "SEEDV_all_features, SEEDV_all_labels = seed_v.get_all_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EEGDataset(data.Dataset):\n",
    "    def __init__(self, features, labels):\n",
    "        super().__init__()\n",
    "        self.features = features\n",
    "        self.labels = labels\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        features, labels = self.features[idx], self.labels[idx]\n",
    "\n",
    "        return features, labels\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.features.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dataset_from_labels(features, labels, class_set): \n",
    "    class_mask = (labels == class_set).any(dim=-1) # reshape class mask [[64], [64],... ] -> [64, 64, ...]\n",
    "    return EEGDataset(features[class_mask], labels[class_mask].reshape((1,-1))[0]) # reshape labels [[0], [1], ...] -> [0, 1, ...]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set = dataset_from_labels(\n",
    "    SEEDV_all_features, SEEDV_all_labels, train_classes)\n",
    "val_set = dataset_from_labels(\n",
    "    SEEDV_all_features, SEEDV_all_labels, val_classes)\n",
    "test_set = dataset_from_labels(\n",
    "    SEEDV_all_features, SEEDV_all_labels, test_classes)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup dataloaders and samplers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FewShotBatchSampler(object):\n",
    "\n",
    "    def __init__(self, dataset_targets, N_way, K_shot, include_query=False, shuffle=True, shuffle_once=False):\n",
    "        \"\"\"\n",
    "        Inputs:\n",
    "            dataset_targets - PyTorch tensor of the labels of the data elements.\n",
    "            N_way - Number of classes to sample per batch.\n",
    "            K_shot - Number of examples to sample per class in the batch.\n",
    "            include_query - If True, returns batch of size N_way*K_shot*2, which \n",
    "                            can be split into support and query set. Simplifies\n",
    "                            the implementation of sampling the same classes but \n",
    "                            distinct examples for support and query set.\n",
    "            shuffle - If True, examples and classes are newly shuffled in each\n",
    "                      iteration (for training)\n",
    "            shuffle_once - If True, examples and classes are shuffled once in \n",
    "                           the beginning, but kept constant across iterations \n",
    "                           (for validation)\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.dataset_targets = dataset_targets\n",
    "        self.N_way = N_way\n",
    "        self.K_shot = K_shot\n",
    "        self.shuffle = shuffle\n",
    "        self.include_query = include_query\n",
    "        if self.include_query:\n",
    "            self.K_shot *= 2\n",
    "        self.batch_size = self.N_way * self.K_shot  # Number of overall images per batch\n",
    "\n",
    "        # Organize examples by class\n",
    "        self.classes = torch.unique(self.dataset_targets).tolist()\n",
    "        self.num_classes = len(self.classes)\n",
    "        self.indices_per_class = {}\n",
    "        self.batches_per_class = {}  # Number of K-shot batches that each class can provide\n",
    "        for c in self.classes:\n",
    "            self.indices_per_class[c] = torch.where(self.dataset_targets == c)[0]\n",
    "            self.batches_per_class[c] = self.indices_per_class[c].shape[0] // self.K_shot\n",
    "\n",
    "        # Create a list of classes from which we select the N classes per batch\n",
    "        self.iterations = sum(self.batches_per_class.values()) // self.N_way\n",
    "        self.class_list = [c for c in self.classes for _ in range(self.batches_per_class[c])]\n",
    "        if shuffle_once or self.shuffle:\n",
    "            self.shuffle_data()\n",
    "        else:\n",
    "            # For testing, we iterate over classes instead of shuffling them\n",
    "            sort_idxs = [i+p*self.num_classes for i,\n",
    "                         c in enumerate(self.classes) for p in range(self.batches_per_class[c])]\n",
    "            self.class_list = np.array(self.class_list)[np.argsort(sort_idxs)].tolist()\n",
    "\n",
    "    def shuffle_data(self):\n",
    "        # Shuffle the examples per class\n",
    "        for c in self.classes:\n",
    "            perm = torch.randperm(self.indices_per_class[c].shape[0])\n",
    "            self.indices_per_class[c] = self.indices_per_class[c][perm]\n",
    "        # Shuffle the class list from which we sample. Note that this way of shuffling\n",
    "        # does not prevent to choose the same class twice in a batch. However, for \n",
    "        # training and validation, this is not a problem.\n",
    "        random.shuffle(self.class_list)\n",
    "\n",
    "    def __iter__(self):\n",
    "        # Shuffle data\n",
    "        if self.shuffle:\n",
    "            self.shuffle_data()\n",
    "\n",
    "        # Sample few-shot batches\n",
    "        start_index = defaultdict(int)\n",
    "        for it in range(self.iterations):\n",
    "            class_batch = self.class_list[it*self.N_way:(it+1)*self.N_way]  # Select N classes for the batch\n",
    "            index_batch = []\n",
    "            for c in class_batch:  # For each class, select the next K examples and add them to the batch\n",
    "                index_batch.extend(self.indices_per_class[c][start_index[c]:start_index[c]+self.K_shot])\n",
    "                start_index[c] += self.K_shot\n",
    "            if self.include_query:  # If we return support+query set, sort them so that they are easy to split\n",
    "                index_batch = index_batch[::2] + index_batch[1::2]\n",
    "            yield index_batch\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.iterations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_WAY = 5 # 5 because 5 classes/emotions.\n",
    "K_SHOT = 4 # how many samples the model can have to learn from per class.\n",
    "train_data_loader = data.DataLoader(train_set,\n",
    "                                    batch_sampler=FewShotBatchSampler(train_set.labels,\n",
    "                                                                      include_query=True,\n",
    "                                                                      N_way=N_WAY,\n",
    "                                                                      K_shot=K_SHOT,\n",
    "                                                                      shuffle=True),\n",
    "                                    num_workers=4)\n",
    "val_data_loader = data.DataLoader(val_set,\n",
    "                                  batch_sampler=FewShotBatchSampler(val_set.labels,\n",
    "                                                                    include_query=True,\n",
    "                                                                    N_way=N_WAY,\n",
    "                                                                    K_shot=K_SHOT,\n",
    "                                                                    shuffle=False,\n",
    "                                                                    shuffle_once=True),\n",
    "                                  num_workers=4)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Split batch into query and support"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_query_support(features, labels):\n",
    "    support_features, query_features = features.chunk(2, dim=0)\n",
    "    support_labels, query_labels = labels.chunk(2, dim=0)\n",
    "    return support_features, query_features, support_labels, query_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "features, labels = next(iter(val_data_loader))\n",
    "support_features, query_features, support_labels, query_labels = split_query_support(features, labels)\n",
    "\n",
    "# fig, ax = plt.subplots(1, 2, figsize=(8, 5))\n",
    "# ax[0].plot(support_features, 'o')\n",
    "# ax[0].set_title(\"Support set\")\n",
    "# ax[0].axis('off')\n",
    "# ax[1].plot(query_features, 'o')\n",
    "# ax[1].set_title(\"Query set\")\n",
    "# ax[1].axis('off')\n",
    "# plt.suptitle(\"Few Shot Batch\", weight='bold')\n",
    "# plt.show()\n",
    "# plt.close()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Normalize and visualize the features in batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalize the features\n",
    "support_features = support_features / support_features.max(0, keepdim=True)[0]\n",
    "query_features = query_features / query_features.max(0, keepdim=True)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 800x500 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "\n",
    "pca = PCA(n_components=2)\n",
    "support_pca = pca.fit_transform(support_features)\n",
    "query_pca = pca.fit_transform(query_features)\n",
    "\n",
    "fig, ax = plt.subplots(1, 2, figsize=(8, 5))\n",
    "ax[0].scatter(support_pca[:,0], support_pca[:,1], c=support_labels)\n",
    "ax[0].set_title(\"Support set\")\n",
    "ax[0].axis('off')\n",
    "ax[1].scatter(query_pca[:,0], query_pca[:,1], c=query_labels)\n",
    "ax[1].set_title(\"Query set\")\n",
    "ax[1].axis('off')\n",
    "plt.suptitle(\"Few Shot Batch\", weight='bold')\n",
    "plt.show()\n",
    "plt.close()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define DCCA neural network used in SEEDV paper"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define CCA methods and classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cca_metric_derivative(H1, H2):\n",
    "    r1 = 1e-3\n",
    "    r2 = 1e-3\n",
    "    eps = 1e-9\n",
    "    # transform the matrix: to be consistent with the original paper\n",
    "    H1 = H1.T\n",
    "    H2 = H2.T\n",
    "    # o1 and o2 are feature dimensions\n",
    "    # m is sample number\n",
    "    o1 = o2 = H1.shape[0]\n",
    "    m = H1.shape[1]\n",
    "\n",
    "    # calculate parameters\n",
    "    H1bar = H1 - H1.mean(axis=1).reshape([-1,1])\n",
    "    H2bar = H2 - H2.mean(axis=1).reshape([-1,1])\n",
    "\n",
    "    SigmaHat12 = (1.0 / (m - 1)) * np.matmul(H1bar, H2bar.T)\n",
    "    SigmaHat11 = (1.0 / (m - 1)) * np.matmul(H1bar, H1bar.T) + r1 * np.eye(o1)\n",
    "    SigmaHat22 = (1.0 / (m - 1)) * np.matmul(H2bar, H2bar.T) + r2 * np.eye(o2)\n",
    "\n",
    "    # eigenvalue and eigenvector decomposition\n",
    "    [D1, V1] = np.linalg.eigh(SigmaHat11)\n",
    "    [D2, V2] = np.linalg.eigh(SigmaHat22)\n",
    "\n",
    "    # remove eighvalues and eigenvectors smaller than 0\n",
    "    posInd1 = np.where(D1 > 0)[0]\n",
    "    D1 = D1[posInd1]\n",
    "    V1 = V1[:, posInd1]\n",
    "\n",
    "    posInd2 = np.where(D2 > 0)[0]\n",
    "    D2 = D2[posInd2]\n",
    "    V2 = V2[:, posInd2]\n",
    "\n",
    "    # calculate matrxi T\n",
    "    SigmaHat11RootInv = np.matmul(np.matmul(V1, np.diag(D1 ** -0.5)), V1.T)\n",
    "    SigmaHat22RootInv = np.matmul(np.matmul(V2, np.diag(D2 ** -0.5)), V2.T)\n",
    "    Tval = np.matmul(np.matmul(SigmaHat11RootInv,SigmaHat12), SigmaHat22RootInv)\n",
    "    # By default, we will use all the singular values\n",
    "    tmp = np.matmul(Tval.T, Tval)\n",
    "    corr = np.sqrt(np.trace(tmp))\n",
    "    cca_loss = -1 * corr\n",
    "\n",
    "    # calculate the derivative of H1 and H2\n",
    "    U_t, D_t, V_prime_t = np.linalg.svd(Tval)\n",
    "    Delta12 = SigmaHat11RootInv @ U_t @ V_prime_t @ SigmaHat22RootInv\n",
    "    Delta11 = SigmaHat11RootInv @ U_t @ np.diag(D_t) @ U_t.T @ SigmaHat11RootInv\n",
    "    Delta22 = SigmaHat22RootInv @ U_t @ np.diag(D_t) @ U_t.T @ SigmaHat22RootInv\n",
    "    Delta11 = -0.5 * Delta11\n",
    "    Delta22 = -0.5 * Delta22\n",
    "\n",
    "    DerivativeH1 = ( 1.0 / (m - 1)) * (2 * (Delta11 @ H1bar) + Delta12 @ H2bar)\n",
    "    DerivativeH2 = ( 1.0 / (m - 1)) * (2 * (Delta22 @ H2bar) + Delta12 @ H1bar)\n",
    "\n",
    "    return cca_loss, DerivativeH1.T, DerivativeH2.T\n",
    "\n",
    "class cca_loss():\n",
    "    def __init__(self, outdim_size, use_all_singular_values, device):\n",
    "        self.outdim_size = outdim_size\n",
    "        self.use_all_singular_values = use_all_singular_values\n",
    "        self.device = device\n",
    "\n",
    "    def loss(self, H1, H2):\n",
    "        r1 = 1e-3\n",
    "        r2 = 1e-3\n",
    "        eps = 1e-9\n",
    "\n",
    "        H1, H2 = H1.t(), H2.t()\n",
    "        o1 = o2 = H1.size(0)\n",
    "\n",
    "        m = H1.size(1)\n",
    "\n",
    "        H1bar = H1 - H1.mean(dim=1).unsqueeze(dim=1)\n",
    "        H2bar = H2 - H2.mean(dim=1).unsqueeze(dim=1)\n",
    "\n",
    "        SigmaHat12 = (1.0 / (m - 1)) * torch.matmul(H1bar, H2bar.t())\n",
    "        SigmaHat11 = (1.0 / (m - 1)) * torch.matmul(H1bar,\n",
    "                                                    H1bar.t()) + r1 * torch.eye(o1, device=self.device)\n",
    "        SigmaHat22 = (1.0 / (m - 1)) * torch.matmul(H2bar,\n",
    "                                                    H2bar.t()) + r2 * torch.eye(o2, device=self.device)\n",
    "\n",
    "        # Calculating the root inverse of covariance matrices by using eigen decomposition\n",
    "        breakpoint()\n",
    "        [D1, V1] = torch.symeig(SigmaHat11, eigenvectors=True)\n",
    "        [D2, V2] = torch.symeig(SigmaHat22, eigenvectors=True)\n",
    "\n",
    "        # Added to increase stability\n",
    "        posInd1 = torch.gt(D1, eps).nonzero()[:, 0]\n",
    "        D1 = D1[posInd1]\n",
    "        V1 = V1[:, posInd1]\n",
    "        posInd2 = torch.gt(D2, eps).nonzero()[:, 0]\n",
    "        D2 = D2[posInd2]\n",
    "        V2 = V2[:, posInd2]\n",
    "\n",
    "        SigmaHat11RootInv = torch.matmul(\n",
    "            torch.matmul(V1, torch.diag(D1 ** -0.5)), V1.t())\n",
    "        SigmaHat22RootInv = torch.matmul(\n",
    "            torch.matmul(V2, torch.diag(D2 ** -0.5)), V2.t())\n",
    "\n",
    "        Tval = torch.matmul(torch.matmul(SigmaHat11RootInv,\n",
    "                                         SigmaHat12), SigmaHat22RootInv)\n",
    "\n",
    "        if self.use_all_singular_values:\n",
    "            # all singular values are used to calculate the correlation\n",
    "            tmp = torch.matmul(Tval.t(), Tval)\n",
    "            corr = torch.trace(torch.sqrt(tmp))\n",
    "            # assert torch.isnan(corr).item() == 0\n",
    "        else:\n",
    "            # just the top self.outdim_size singular values are used\n",
    "            trace_TT = torch.matmul(Tval.t(), Tval)\n",
    "            trace_TT = torch.add(trace_TT, (torch.eye(trace_TT.shape[0])*r1).to(self.device)) # regularization for more stability\n",
    "            U, V = torch.symeig(trace_TT, eigenvectors=True)\n",
    "            U = torch.where(U>eps, U, (torch.ones(U.shape)*eps).to(self.device))\n",
    "            U = U.topk(self.outdim_size)[0]\n",
    "            corr = torch.sum(torch.sqrt(U))\n",
    "        return -corr"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define DCCA network layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformLayers(nn.Module):\n",
    "    def __init__(self, input_size, layer_sizes):\n",
    "        super(TransformLayers, self).__init__()\n",
    "        layers = []\n",
    "        layer_sizes = [input_size] + layer_sizes\n",
    "        for l_id in range(len(layer_sizes) - 1):\n",
    "            if l_id == len(layer_sizes) - 2:\n",
    "                layers.append(nn.Sequential(\n",
    "                    #nn.BatchNorm1d(num_features=layer_sizes[l_id], affine=False),\n",
    "                    nn.Linear(layer_sizes[l_id], layer_sizes[l_id+1]),\n",
    "                    ))\n",
    "            else:\n",
    "                layers.append(nn.Sequential(\n",
    "                    nn.Linear(layer_sizes[l_id], layer_sizes[l_id+1]),\n",
    "                    nn.Sigmoid(),\n",
    "                    #nn.BatchNorm1d(num_features=layer_sizes[l_id+1], affine=False),\n",
    "                    ))\n",
    "        self.layers = nn.ModuleList(layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        for layer in self.layers:\n",
    "            x = layer(x)\n",
    "        return x\n",
    "\n",
    "class AttentionFusion(nn.Module):\n",
    "    def __init__(self, output_dim):\n",
    "        super(AttentionFusion, self).__init__()\n",
    "        self.output_dim = output_dim\n",
    "        self.attention_weights = nn.Parameter(torch.randn(self.output_dim, requires_grad=True))\n",
    "    def forward(self, x1, x2):\n",
    "        # calculate weigths for all input samples\n",
    "        row, _ = x1.shape\n",
    "        fused_tensor = torch.empty_like(x1)\n",
    "        alpha = []\n",
    "        for i in range(row):\n",
    "            tmp1 = torch.dot(x1[i,:], self.attention_weights)\n",
    "            tmp2 = torch.dot(x2[i,:], self.attention_weights)\n",
    "            alpha_1 = torch.exp(tmp1) / (torch.exp(tmp1) + torch.exp(tmp2))\n",
    "            alpha_2 = 1 - alpha_1\n",
    "            alpha.append((alpha_1.detach().cpu().numpy(), alpha_2.detach().cpu().numpy()))\n",
    "            fused_tensor[i, :] = alpha_1 * x1[i,:] + alpha_2 * x2[i, :]\n",
    "        return fused_tensor, alpha\n",
    "\n",
    "class DCCA_AM(nn.Module):\n",
    "    def __init__(self, input_size1, input_size2, layer_sizes1, layer_sizes2, outdim_size, categories, device):\n",
    "        super(DCCA_AM, self).__init__()\n",
    "        self.input_dim_split = input_size1\n",
    "        self.outdim_size = outdim_size\n",
    "        self.categories = categories\n",
    "        # self.use_all_singular_values = use_all_singular_values\n",
    "        self.device = device\n",
    "\n",
    "        self.model1 = TransformLayers(input_size1, layer_sizes1).to(self.device)\n",
    "        self.model2 = TransformLayers(input_size2, layer_sizes2).to(self.device)\n",
    "\n",
    "        # convert generator object to list for deepcopy(model) to work\n",
    "        self.model1_parameters = list(self.model1.parameters()) \n",
    "        self.model2_parameters = list(self.model1.parameters())\n",
    "\n",
    "        self.classification = nn.Linear(self.outdim_size, self.categories)\n",
    "\n",
    "        self.attention_fusion = AttentionFusion(outdim_size)\n",
    "    def forward(self, x):\n",
    "        x1 = x[:, :self.input_dim_split]\n",
    "        x2 = x[:, self.input_dim_split:]\n",
    "        # forward process: returns negative of cca loss and predicted labels\n",
    "        output1 = self.model1(x1)\n",
    "        output2 = self.model2(x2)\n",
    "        # cca_loss_val = self.loss(output1, output2)\n",
    "        cca_loss, partial_h1, partial_h2 = cca_metric_derivative(output1.detach().cpu().numpy(), output2.detach().cpu().numpy())\n",
    "        fused_tensor, alpha = self.attention_fusion(output1, output2)\n",
    "        out = self.classification(fused_tensor)\n",
    "        return out, cca_loss, output1, output2, partial_h1, partial_h2, fused_tensor.detach().cpu().data, alpha"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define meta learning model and methods"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define baseline ProtoNet model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ProtoNet(pl.LightningModule):\n",
    "\n",
    "    def __init__(self, lr, model_args):\n",
    "        \"\"\"\n",
    "        Inputs\n",
    "            proto_dim - Dimensionality of prototype feature space\n",
    "            lr - Learning rate of Adam optimizer\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.save_hyperparameters()\n",
    "        eeg_input_dim, eye_input_dim, layer_sizes1, layer_sizes2, \\\n",
    "            output_dim, num_emotions, device = model_args\n",
    "        self.model = DCCA_AM(eeg_input_dim, eye_input_dim, layer_sizes1, layer_sizes2, output_dim, num_emotions, device).to(device)\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        optimizer = optim.AdamW(self.parameters(), lr=self.hparams.lr)\n",
    "        scheduler = optim.lr_scheduler.MultiStepLR(\n",
    "            optimizer, milestones=[140, 180], gamma=0.1)\n",
    "        return [optimizer], [scheduler]\n",
    "\n",
    "    @staticmethod\n",
    "    def calculate_prototypes(features, targets):\n",
    "        # Given a stack of features vectors and labels, return class prototypes\n",
    "        # features - shape [N, proto_dim], targets - shape [N]\n",
    "        features = features[0]\n",
    "        targets = targets.reshape((1,-1))[0]\n",
    "        classes, _ = torch.unique(targets).sort() # Determine which classes we have\n",
    "        prototypes = []\n",
    "        # print(\"targets:\", targets)\n",
    "        for c in classes:\n",
    "            # print(\"c:\", c)\n",
    "            # print(features[torch.where(targets == c)[0]])\n",
    "            # maybe use for target in targets loop\n",
    "            p = features[torch.where(targets == c)[0]].mean(dim=0)  # Average class feature vectors\n",
    "            prototypes.append(p)\n",
    "        prototypes = torch.stack(prototypes, dim=0)\n",
    "        # Return the 'classes' tensor to know which prototype belongs to which class\n",
    "        return prototypes, classes\n",
    "\n",
    "    def classify_feats(self, prototypes, classes, feats, targets):\n",
    "        # Classify new examples with prototypes and return classification error\n",
    "        dist = torch.pow(prototypes[None, :] - feats[:, None], 2).sum(dim=2)  # Squared euclidean distance\n",
    "        preds = F.log_softmax(-dist, dim=1)\n",
    "        labels = (classes[None, :] == targets[:, None]).long().argmax(dim=-1)\n",
    "        acc = (preds.argmax(dim=1) == labels).float().mean()\n",
    "        return preds, labels, acc\n",
    "\n",
    "    def calculate_loss(self, batch, mode):\n",
    "        # Determine training loss for a given support and query set \n",
    "        imgs, targets = batch\n",
    "        features = self.model(imgs)  # Encode all images of support and query set\n",
    "        support_feats, query_feats, support_targets, query_targets = split_query_support(features, targets)\n",
    "        prototypes, classes = ProtoNet.calculate_prototypes(support_feats, support_targets)\n",
    "        preds, labels, acc = self.classify_feats(prototypes, classes, query_feats, query_targets)\n",
    "        loss = F.cross_entropy(preds, labels)\n",
    "\n",
    "        self.log(f\"{mode}_loss\", loss)\n",
    "        self.log(f\"{mode}_acc\", acc)\n",
    "        return loss\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        return self.calculate_loss(batch, mode=\"train\")\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        _ = self.calculate_loss(batch, mode=\"val\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define ProtoMAML model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ProtoMAML(pl.LightningModule):\n",
    "    \n",
    "    def __init__(self, lr, lr_inner, lr_output, num_inner_steps, model_args):\n",
    "        \"\"\"\n",
    "        Inputs\n",
    "            eeg_input_dim - Dimensionality of prototype feature space\n",
    "            lr - Learning rate of the outer loop Adam optimizer\n",
    "            lr_inner - Learning rate of the inner loop SGD optimizer\n",
    "            lr_output - Learning rate for the output layer in the inner loop\n",
    "            num_inner_steps - Number of inner loop updates to perform\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.save_hyperparameters()\n",
    "        eeg_input_dim, eye_input_dim, layer_sizes1, layer_sizes2, \\\n",
    "            output_dim, num_emotions, device = model_args\n",
    "        self.model = DCCA_AM(eeg_input_dim, eye_input_dim, layer_sizes1, layer_sizes2, output_dim, num_emotions, device).to(device)\n",
    "        \n",
    "    def configure_optimizers(self):\n",
    "        optimizer = optim.AdamW(self.parameters(), lr=self.hparams.lr)\n",
    "        scheduler = optim.lr_scheduler.MultiStepLR(optimizer, milestones=[140,180], gamma=0.1)\n",
    "        return [optimizer], [scheduler]\n",
    "        \n",
    "    def run_model(self, local_model, output_weight, output_bias, imgs, labels):\n",
    "        # Execute a model with given output layer weights and inputs\n",
    "        feats = local_model(imgs)\n",
    "        # get only first element of tuple in feats\n",
    "        preds = F.linear(feats[0], output_weight, output_bias)\n",
    "        loss = F.cross_entropy(preds, labels)\n",
    "        acc = (preds.argmax(dim=1) == labels).float()\n",
    "        return loss, preds, acc\n",
    "        \n",
    "    def adapt_few_shot(self, support_imgs, support_targets):\n",
    "        # Determine prototype initialization\n",
    "        support_feats = self.model(support_imgs)\n",
    "        prototypes, classes = ProtoNet.calculate_prototypes(support_feats, support_targets)\n",
    "        support_labels = (classes[None,:] == support_targets[:,None]).long().argmax(dim=-1)\n",
    "        # Create inner-loop model and optimizer\n",
    "        local_model = deepcopy(self.model)\n",
    "        local_model.train()\n",
    "        local_optim = optim.SGD(local_model.parameters(), lr=self.hparams.lr_inner)\n",
    "        local_optim.zero_grad()\n",
    "        # Create output layer weights with prototype-based initialization\n",
    "        init_weight = 2 * prototypes\n",
    "        init_bias = -torch.norm(prototypes, dim=1)**2\n",
    "        output_weight = init_weight.detach().requires_grad_()\n",
    "        output_bias = init_bias.detach().requires_grad_()\n",
    "        \n",
    "        # Optimize inner loop model on support set\n",
    "        for _ in range(self.hparams.num_inner_steps):\n",
    "            # Determine loss on the support set\n",
    "            loss, _, _ = self.run_model(local_model, output_weight, output_bias, support_imgs, support_labels)\n",
    "            # Calculate gradients and perform inner loop update\n",
    "            loss.backward()\n",
    "            local_optim.step()\n",
    "            # Update output layer via SGD\n",
    "            output_weight.data -= self.hparams.lr_output * output_weight.grad\n",
    "            output_bias.data -= self.hparams.lr_output * output_bias.grad\n",
    "            # Reset gradients\n",
    "            local_optim.zero_grad()\n",
    "            output_weight.grad.fill_(0)\n",
    "            output_bias.grad.fill_(0)\n",
    "            \n",
    "        # Re-attach computation graph of prototypes\n",
    "        output_weight = (output_weight - init_weight).detach() + init_weight\n",
    "        output_bias = (output_bias - init_bias).detach() + init_bias\n",
    "        \n",
    "        return local_model, output_weight, output_bias, classes\n",
    "        \n",
    "    def outer_loop(self, batch, mode=\"train\"):\n",
    "        accuracies = []\n",
    "        losses = []\n",
    "        self.model.zero_grad()\n",
    "        \n",
    "        # Determine gradients for batch of tasks\n",
    "        for task_batch in batch:\n",
    "            imgs, targets = task_batch\n",
    "            support_imgs, query_imgs, support_targets, query_targets = split_query_support(imgs, targets)\n",
    "            # Perform inner loop adaptation\n",
    "            local_model, output_weight, output_bias, classes = self.adapt_few_shot(support_imgs, support_targets)\n",
    "            # Determine loss of query set\n",
    "            query_labels = (classes[None,:] == query_targets[:,None]).long().argmax(dim=-1)\n",
    "            loss, preds, acc = self.run_model(local_model, output_weight, output_bias, query_imgs, query_labels)\n",
    "            # Calculate gradients for query set loss\n",
    "            if mode == \"train\":\n",
    "                loss.backward()\n",
    "\n",
    "                for p_global, p_local in zip(self.model.parameters(), local_model.parameters()):\n",
    "                    p_global.grad += p_local.grad  # First-order approx. -> add gradients of finetuned and base model\n",
    "            \n",
    "            accuracies.append(acc.mean().detach())\n",
    "            losses.append(loss.detach())\n",
    "        \n",
    "        # Perform update of base model\n",
    "        if mode == \"train\":\n",
    "            opt = self.optimizers()\n",
    "            opt.step()\n",
    "            opt.zero_grad()\n",
    "        \n",
    "        self.log(f\"{mode}_loss\", sum(losses) / len(losses))\n",
    "        self.log(f\"{mode}_acc\", sum(accuracies) / len(accuracies))\n",
    "    \n",
    "    def training_step(self, batch, batch_idx):\n",
    "        self.outer_loop(batch, mode=\"train\")\n",
    "        return None  # Returning None means we skip the default training optimizer steps by PyTorch Lightning\n",
    "        \n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        # Validation requires to finetune a model, hence we need to enable gradients\n",
    "        torch.set_grad_enabled(True)\n",
    "        self.outer_loop(batch, mode=\"val\")\n",
    "        torch.set_grad_enabled(False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define task batch sampler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TaskBatchSampler(object):\n",
    "    \n",
    "    def __init__(self, dataset_targets, batch_size, N_way, K_shot, include_query=False, shuffle=True):\n",
    "        \"\"\"\n",
    "        Inputs:\n",
    "            dataset_targets - PyTorch tensor of the labels of the data elements.\n",
    "            batch_size - Number of tasks to aggregate in a batch\n",
    "            N_way - Number of classes to sample per batch.\n",
    "            K_shot - Number of examples to sample per class in the batch.\n",
    "            include_query - If True, returns batch of size N_way*K_shot*2, which \n",
    "                            can be split into support and query set. Simplifies\n",
    "                            the implementation of sampling the same classes but \n",
    "                            distinct examples for support and query set.\n",
    "            shuffle - If True, examples and classes are newly shuffled in each\n",
    "                      iteration (for training)\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.batch_sampler = FewShotBatchSampler(dataset_targets, N_way, K_shot, include_query, shuffle)\n",
    "        self.task_batch_size = batch_size\n",
    "        self.local_batch_size = self.batch_sampler.batch_size\n",
    "        \n",
    "    def __iter__(self):\n",
    "        # Aggregate multiple batches before returning the indices\n",
    "        batch_list = []\n",
    "        for batch_idx, batch in enumerate(self.batch_sampler):\n",
    "            batch_list.extend(batch)\n",
    "            if (batch_idx+1) % self.task_batch_size == 0:\n",
    "                yield batch_list\n",
    "                batch_list = []\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.batch_sampler)//self.task_batch_size\n",
    "    \n",
    "    def get_collate_fn(self):\n",
    "        # Returns a collate function that converts one big tensor into a list of task-specific tensors\n",
    "        def collate_fn(item_list):\n",
    "            imgs = torch.stack([img for img, target in item_list], dim=0)\n",
    "            targets = torch.stack([target for img, target in item_list], dim=0)\n",
    "            imgs = imgs.chunk(self.task_batch_size, dim=0)\n",
    "            targets = targets.chunk(self.task_batch_size, dim=0)\n",
    "            return list(zip(imgs, targets))\n",
    "        return collate_fn"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define model training and testing methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model_class, train_loader, val_loader, **kwargs):\n",
    "    trainer = pl.Trainer(default_root_dir=os.path.join(CHECKPOINT_PATH, model_class.__name__),\n",
    "                         accelerator=\"gpu\" if str(device).startswith(\"cuda\") else \"cpu\",\n",
    "                         devices=1,\n",
    "                         max_epochs=200,\n",
    "                         callbacks=[ModelCheckpoint(save_weights_only=True, mode=\"max\", monitor=\"val_acc\"),\n",
    "                                    LearningRateMonitor(\"epoch\")],\n",
    "                         enable_progress_bar=False)\n",
    "    trainer.logger._default_hp_metric = None\n",
    "\n",
    "    # Check whether pretrained model exists. If yes, load it and skip training\n",
    "    pretrained_filename = os.path.join(\n",
    "        CHECKPOINT_PATH, model_class.__name__ + \".ckpt\")\n",
    "    if os.path.isfile(pretrained_filename):\n",
    "        print(f\"Found pretrained model at {pretrained_filename}, loading...\")\n",
    "        # Automatically loads the model with the saved hyperparameters\n",
    "        model = model_class.load_from_checkpoint(pretrained_filename)\n",
    "    else:\n",
    "        pl.seed_everything(42)  # To be reproducable\n",
    "        model = model_class(**kwargs)\n",
    "        trainer.fit(model, train_loader, val_loader)\n",
    "        model = model_class.load_from_checkpoint(\n",
    "            trainer.checkpoint_callback.best_model_path)  # Load best checkpoint after training\n",
    "\n",
    "    return model\n",
    "\n",
    "def test_protomaml(model, dataset, k_shot=4):\n",
    "    pl.seed_everything(42)\n",
    "    model = model.to(device)\n",
    "    num_classes = dataset.targets.unique().shape[0]\n",
    "    exmps_per_class = dataset.targets.shape[0]//num_classes\n",
    "    \n",
    "    # Data loader for full test set as query set\n",
    "    full_dataloader = data.DataLoader(dataset, \n",
    "                                      batch_size=128, \n",
    "                                      num_workers=4, \n",
    "                                      shuffle=False, \n",
    "                                      drop_last=False)\n",
    "    # Data loader for sampling support sets\n",
    "    sampler = FewShotBatchSampler(dataset.targets, \n",
    "                                  include_query=False,\n",
    "                                  N_way=num_classes,\n",
    "                                  K_shot=k_shot,\n",
    "                                  shuffle=False,\n",
    "                                  shuffle_once=False)\n",
    "    sample_dataloader = data.DataLoader(dataset, \n",
    "                                        batch_sampler=sampler,\n",
    "                                        num_workers=2)\n",
    "    \n",
    "    # We iterate through the full dataset in two manners. First, to select the k-shot batch. \n",
    "    # Second, the evaluate the model on all other examples\n",
    "    accuracies = []\n",
    "    for (support_imgs, support_targets), support_indices in tqdm(zip(sample_dataloader, sampler), \"Performing few-shot finetuning\"):\n",
    "        support_imgs = support_imgs.to(device)\n",
    "        support_targets = support_targets.to(device)\n",
    "        # Finetune new model on support set\n",
    "        local_model, output_weight, output_bias, classes = model.adapt_few_shot(support_imgs, support_targets)\n",
    "        with torch.no_grad():  # No gradients for query set needed\n",
    "            local_model.eval()\n",
    "            batch_acc = torch.zeros((0,), dtype=torch.float32, device=device)\n",
    "            # Evaluate all examples in test dataset\n",
    "            for query_imgs, query_targets in full_dataloader:\n",
    "                query_imgs = query_imgs.to(device)\n",
    "                query_targets = query_targets.to(device)\n",
    "                query_labels = (classes[None,:] == query_targets[:,None]).long().argmax(dim=-1)\n",
    "                _, _, acc = model.run_model(local_model, output_weight, output_bias, query_imgs, query_labels)\n",
    "                batch_acc = torch.cat([batch_acc, acc.detach()], dim=0)\n",
    "            # Exclude support set elements\n",
    "            for s_idx in support_indices:\n",
    "                batch_acc[s_idx] = 0\n",
    "            batch_acc = batch_acc.sum().item() / (batch_acc.shape[0] - len(support_indices))\n",
    "            accuracies.append(batch_acc)\n",
    "    return mean(accuracies), stdev(accuracies)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train meta learning model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training constant (same as for ProtoNet)\n",
    "N_WAY = 5\n",
    "K_SHOT = 4\n",
    "\n",
    "# Training set\n",
    "train_protomaml_sampler = TaskBatchSampler(train_set.labels, \n",
    "                                           include_query=True,\n",
    "                                           N_way=N_WAY,\n",
    "                                           K_shot=K_SHOT,\n",
    "                                           batch_size=16)\n",
    "train_protomaml_loader = data.DataLoader(train_set, \n",
    "                                         batch_sampler=train_protomaml_sampler,\n",
    "                                         collate_fn=train_protomaml_sampler.get_collate_fn(),\n",
    "                                         num_workers=2)\n",
    "\n",
    "# Validation set\n",
    "val_protomaml_sampler = TaskBatchSampler(val_set.labels, \n",
    "                                         include_query=True,\n",
    "                                         N_way=N_WAY,\n",
    "                                         K_shot=K_SHOT,\n",
    "                                         batch_size=1,  # We do not update the parameters, hence the batch size is irrelevant here\n",
    "                                         shuffle=False)\n",
    "val_protomaml_loader = data.DataLoader(val_set, \n",
    "                                       batch_sampler=val_protomaml_sampler,\n",
    "                                       collate_fn=val_protomaml_sampler.get_collate_fn(),\n",
    "                                       num_workers=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "Global seed set to 42\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name  | Type    | Params\n",
      "----------------------------------\n",
      "0 | model | DCCA_AM | 67.9 K\n",
      "----------------------------------\n",
      "67.9 K    Trainable params\n",
      "0         Non-trainable params\n",
      "67.9 K    Total params\n",
      "0.272     Total estimated model params size (MB)\n",
      "/home/chanel/miniconda3/envs/pytorch-lightning-env/lib/python3.9/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:224: PossibleUserWarning: The dataloader, val_dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 48 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "/home/chanel/miniconda3/envs/pytorch-lightning-env/lib/python3.9/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:224: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 48 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "/home/chanel/miniconda3/envs/pytorch-lightning-env/lib/python3.9/site-packages/pytorch_lightning/trainer/trainer.py:1558: PossibleUserWarning: The number of training batches (36) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "  rank_zero_warn(\n",
      "/home/chanel/miniconda3/envs/pytorch-lightning-env/lib/python3.9/site-packages/pytorch_lightning/loops/optimization/optimizer_loop.py:136: UserWarning: `training_step` returned `None`. If this was on purpose, ignore this warning...\n",
      "  self.warning_cache.warn(\"`training_step` returned `None`. If this was on purpose, ignore this warning...\")\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "setStorage: sizes [], strides [], storage offset 13, and itemsize 8 requiring a storage size of 112 are out of bounds for storage of size 0",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[23], line 7\u001b[0m\n\u001b[1;32m      4\u001b[0m LAYER_SIZES \u001b[39m=\u001b[39m [np\u001b[39m.\u001b[39mrandom\u001b[39m.\u001b[39mrandint(\u001b[39m100\u001b[39m,\u001b[39m200\u001b[39m), np\u001b[39m.\u001b[39mrandom\u001b[39m.\u001b[39mrandint(\u001b[39m20\u001b[39m,\u001b[39m50\u001b[39m), OUTPUT_DIM]\n\u001b[1;32m      5\u001b[0m NUM_EMOTIONS \u001b[39m=\u001b[39m N_WAY\n\u001b[0;32m----> 7\u001b[0m protomaml_model \u001b[39m=\u001b[39m train_model(ProtoMAML, \n\u001b[1;32m      8\u001b[0m                               lr\u001b[39m=\u001b[39;49m\u001b[39m1e-3\u001b[39;49m, \n\u001b[1;32m      9\u001b[0m                               lr_inner\u001b[39m=\u001b[39;49m\u001b[39m0.1\u001b[39;49m,\n\u001b[1;32m     10\u001b[0m                               lr_output\u001b[39m=\u001b[39;49m\u001b[39m0.1\u001b[39;49m,\n\u001b[1;32m     11\u001b[0m                               num_inner_steps\u001b[39m=\u001b[39;49m\u001b[39m1\u001b[39;49m,\n\u001b[1;32m     12\u001b[0m                               model_args \u001b[39m=\u001b[39;49m (EEG_INPUT_DIM, EYE_INPUT_DIM, LAYER_SIZES,\n\u001b[1;32m     13\u001b[0m                                 LAYER_SIZES, OUTPUT_DIM, NUM_EMOTIONS, device),\n\u001b[1;32m     14\u001b[0m                               train_loader\u001b[39m=\u001b[39;49mtrain_protomaml_loader, \n\u001b[1;32m     15\u001b[0m                               val_loader\u001b[39m=\u001b[39;49mval_protomaml_loader)\n",
      "Cell \u001b[0;32mIn[21], line 21\u001b[0m, in \u001b[0;36mtrain_model\u001b[0;34m(model_class, train_loader, val_loader, **kwargs)\u001b[0m\n\u001b[1;32m     19\u001b[0m     pl\u001b[39m.\u001b[39mseed_everything(\u001b[39m42\u001b[39m)  \u001b[39m# To be reproducable\u001b[39;00m\n\u001b[1;32m     20\u001b[0m     model \u001b[39m=\u001b[39m model_class(\u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m---> 21\u001b[0m     trainer\u001b[39m.\u001b[39;49mfit(model, train_loader, val_loader)\n\u001b[1;32m     22\u001b[0m     model \u001b[39m=\u001b[39m model_class\u001b[39m.\u001b[39mload_from_checkpoint(\n\u001b[1;32m     23\u001b[0m         trainer\u001b[39m.\u001b[39mcheckpoint_callback\u001b[39m.\u001b[39mbest_model_path)  \u001b[39m# Load best checkpoint after training\u001b[39;00m\n\u001b[1;32m     25\u001b[0m \u001b[39mreturn\u001b[39;00m model\n",
      "File \u001b[0;32m~/miniconda3/envs/pytorch-lightning-env/lib/python3.9/site-packages/pytorch_lightning/trainer/trainer.py:582\u001b[0m, in \u001b[0;36mTrainer.fit\u001b[0;34m(self, model, train_dataloaders, val_dataloaders, datamodule, ckpt_path)\u001b[0m\n\u001b[1;32m    580\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mTypeError\u001b[39;00m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m`Trainer.fit()` requires a `LightningModule`, got: \u001b[39m\u001b[39m{\u001b[39;00mmodel\u001b[39m.\u001b[39m\u001b[39m__class__\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__qualname__\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n\u001b[1;32m    581\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstrategy\u001b[39m.\u001b[39m_lightning_module \u001b[39m=\u001b[39m model\n\u001b[0;32m--> 582\u001b[0m call\u001b[39m.\u001b[39;49m_call_and_handle_interrupt(\n\u001b[1;32m    583\u001b[0m     \u001b[39mself\u001b[39;49m, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_fit_impl, model, train_dataloaders, val_dataloaders, datamodule, ckpt_path\n\u001b[1;32m    584\u001b[0m )\n",
      "File \u001b[0;32m~/miniconda3/envs/pytorch-lightning-env/lib/python3.9/site-packages/pytorch_lightning/trainer/call.py:38\u001b[0m, in \u001b[0;36m_call_and_handle_interrupt\u001b[0;34m(trainer, trainer_fn, *args, **kwargs)\u001b[0m\n\u001b[1;32m     36\u001b[0m         \u001b[39mreturn\u001b[39;00m trainer\u001b[39m.\u001b[39mstrategy\u001b[39m.\u001b[39mlauncher\u001b[39m.\u001b[39mlaunch(trainer_fn, \u001b[39m*\u001b[39margs, trainer\u001b[39m=\u001b[39mtrainer, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[1;32m     37\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m---> 38\u001b[0m         \u001b[39mreturn\u001b[39;00m trainer_fn(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m     40\u001b[0m \u001b[39mexcept\u001b[39;00m _TunerExitException:\n\u001b[1;32m     41\u001b[0m     trainer\u001b[39m.\u001b[39m_call_teardown_hook()\n",
      "File \u001b[0;32m~/miniconda3/envs/pytorch-lightning-env/lib/python3.9/site-packages/pytorch_lightning/trainer/trainer.py:624\u001b[0m, in \u001b[0;36mTrainer._fit_impl\u001b[0;34m(self, model, train_dataloaders, val_dataloaders, datamodule, ckpt_path)\u001b[0m\n\u001b[1;32m    617\u001b[0m ckpt_path \u001b[39m=\u001b[39m ckpt_path \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mresume_from_checkpoint\n\u001b[1;32m    618\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_ckpt_path \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_checkpoint_connector\u001b[39m.\u001b[39m_set_ckpt_path(\n\u001b[1;32m    619\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstate\u001b[39m.\u001b[39mfn,\n\u001b[1;32m    620\u001b[0m     ckpt_path,  \u001b[39m# type: ignore[arg-type]\u001b[39;00m\n\u001b[1;32m    621\u001b[0m     model_provided\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m,\n\u001b[1;32m    622\u001b[0m     model_connected\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlightning_module \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m,\n\u001b[1;32m    623\u001b[0m )\n\u001b[0;32m--> 624\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_run(model, ckpt_path\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mckpt_path)\n\u001b[1;32m    626\u001b[0m \u001b[39massert\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstate\u001b[39m.\u001b[39mstopped\n\u001b[1;32m    627\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtraining \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/pytorch-lightning-env/lib/python3.9/site-packages/pytorch_lightning/trainer/trainer.py:1061\u001b[0m, in \u001b[0;36mTrainer._run\u001b[0;34m(self, model, ckpt_path)\u001b[0m\n\u001b[1;32m   1057\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_checkpoint_connector\u001b[39m.\u001b[39mrestore_training_state()\n\u001b[1;32m   1059\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_checkpoint_connector\u001b[39m.\u001b[39mresume_end()\n\u001b[0;32m-> 1061\u001b[0m results \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_run_stage()\n\u001b[1;32m   1063\u001b[0m log\u001b[39m.\u001b[39mdetail(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__class__\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m: trainer tearing down\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m   1064\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_teardown()\n",
      "File \u001b[0;32m~/miniconda3/envs/pytorch-lightning-env/lib/python3.9/site-packages/pytorch_lightning/trainer/trainer.py:1140\u001b[0m, in \u001b[0;36mTrainer._run_stage\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1138\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpredicting:\n\u001b[1;32m   1139\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_run_predict()\n\u001b[0;32m-> 1140\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_run_train()\n",
      "File \u001b[0;32m~/miniconda3/envs/pytorch-lightning-env/lib/python3.9/site-packages/pytorch_lightning/trainer/trainer.py:1163\u001b[0m, in \u001b[0;36mTrainer._run_train\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1160\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfit_loop\u001b[39m.\u001b[39mtrainer \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\n\u001b[1;32m   1162\u001b[0m \u001b[39mwith\u001b[39;00m torch\u001b[39m.\u001b[39mautograd\u001b[39m.\u001b[39mset_detect_anomaly(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_detect_anomaly):\n\u001b[0;32m-> 1163\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mfit_loop\u001b[39m.\u001b[39;49mrun()\n",
      "File \u001b[0;32m~/miniconda3/envs/pytorch-lightning-env/lib/python3.9/site-packages/pytorch_lightning/loops/loop.py:199\u001b[0m, in \u001b[0;36mLoop.run\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    197\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m    198\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mon_advance_start(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m--> 199\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49madvance(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    200\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mon_advance_end()\n\u001b[1;32m    201\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_restarting \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/pytorch-lightning-env/lib/python3.9/site-packages/pytorch_lightning/loops/fit_loop.py:267\u001b[0m, in \u001b[0;36mFitLoop.advance\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    265\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_data_fetcher\u001b[39m.\u001b[39msetup(dataloader, batch_to_device\u001b[39m=\u001b[39mbatch_to_device)\n\u001b[1;32m    266\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtrainer\u001b[39m.\u001b[39mprofiler\u001b[39m.\u001b[39mprofile(\u001b[39m\"\u001b[39m\u001b[39mrun_training_epoch\u001b[39m\u001b[39m\"\u001b[39m):\n\u001b[0;32m--> 267\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mepoch_loop\u001b[39m.\u001b[39;49mrun(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_data_fetcher)\n",
      "File \u001b[0;32m~/miniconda3/envs/pytorch-lightning-env/lib/python3.9/site-packages/pytorch_lightning/loops/loop.py:194\u001b[0m, in \u001b[0;36mLoop.run\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    190\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mon_skip()\n\u001b[1;32m    192\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mreset()\n\u001b[0;32m--> 194\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mon_run_start(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    196\u001b[0m \u001b[39mwhile\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdone:\n\u001b[1;32m    197\u001b[0m     \u001b[39mtry\u001b[39;00m:\n",
      "File \u001b[0;32m~/miniconda3/envs/pytorch-lightning-env/lib/python3.9/site-packages/pytorch_lightning/loops/epoch/training_epoch_loop.py:161\u001b[0m, in \u001b[0;36mTrainingEpochLoop.on_run_start\u001b[0;34m(self, data_fetcher)\u001b[0m\n\u001b[1;32m    159\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mon_run_start\u001b[39m(\u001b[39mself\u001b[39m, data_fetcher: AbstractDataFetcher) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    160\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_reload_dataloader_state_dict(data_fetcher)\n\u001b[0;32m--> 161\u001b[0m     _ \u001b[39m=\u001b[39m \u001b[39miter\u001b[39;49m(data_fetcher)  \u001b[39m# creates the iterator inside the fetcher\u001b[39;00m\n\u001b[1;32m    162\u001b[0m     \u001b[39m# add the previous `fetched` value to properly track `is_last_batch` with no prefetching\u001b[39;00m\n\u001b[1;32m    163\u001b[0m     data_fetcher\u001b[39m.\u001b[39mfetched \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbatch_progress\u001b[39m.\u001b[39mcurrent\u001b[39m.\u001b[39mready\n",
      "File \u001b[0;32m~/miniconda3/envs/pytorch-lightning-env/lib/python3.9/site-packages/pytorch_lightning/utilities/fetching.py:179\u001b[0m, in \u001b[0;36mAbstractDataFetcher.__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    177\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mreset()\n\u001b[1;32m    178\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataloader_iter \u001b[39m=\u001b[39m \u001b[39miter\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataloader)\n\u001b[0;32m--> 179\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_apply_patch()\n\u001b[1;32m    180\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mprefetching()\n\u001b[1;32m    181\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\n",
      "File \u001b[0;32m~/miniconda3/envs/pytorch-lightning-env/lib/python3.9/site-packages/pytorch_lightning/utilities/fetching.py:120\u001b[0m, in \u001b[0;36mAbstractDataFetcher._apply_patch\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    117\u001b[0m         loader\u001b[39m.\u001b[39m_lightning_fetcher \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\n\u001b[1;32m    118\u001b[0m         patch_dataloader_iterator(loader, iterator, \u001b[39mself\u001b[39m)\n\u001b[0;32m--> 120\u001b[0m apply_to_collections(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mloaders, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mloader_iters, (Iterator, DataLoader), _apply_patch_fn)\n",
      "File \u001b[0;32m~/miniconda3/envs/pytorch-lightning-env/lib/python3.9/site-packages/pytorch_lightning/utilities/fetching.py:156\u001b[0m, in \u001b[0;36mAbstractDataFetcher.loader_iters\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    154\u001b[0m     \u001b[39mraise\u001b[39;00m MisconfigurationException(\u001b[39m\"\u001b[39m\u001b[39mThe `dataloader_iter` isn\u001b[39m\u001b[39m'\u001b[39m\u001b[39mt available outside the __iter__ context.\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m    155\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataloader, CombinedLoader):\n\u001b[0;32m--> 156\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdataloader_iter\u001b[39m.\u001b[39;49mloader_iters\n\u001b[1;32m    157\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataloader_iter\n",
      "File \u001b[0;32m~/miniconda3/envs/pytorch-lightning-env/lib/python3.9/site-packages/pytorch_lightning/trainer/supporters.py:555\u001b[0m, in \u001b[0;36mCombinedLoaderIterator.loader_iters\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    553\u001b[0m \u001b[39m\"\"\"Get the `_loader_iters` and create one if it is None.\"\"\"\u001b[39;00m\n\u001b[1;32m    554\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_loader_iters \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m--> 555\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_loader_iters \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcreate_loader_iters(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mloaders)\n\u001b[1;32m    557\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_loader_iters\n",
      "File \u001b[0;32m~/miniconda3/envs/pytorch-lightning-env/lib/python3.9/site-packages/pytorch_lightning/trainer/supporters.py:595\u001b[0m, in \u001b[0;36mCombinedLoaderIterator.create_loader_iters\u001b[0;34m(loaders)\u001b[0m\n\u001b[1;32m    586\u001b[0m \u001b[39m\"\"\"Create and return a collection of iterators from loaders.\u001b[39;00m\n\u001b[1;32m    587\u001b[0m \n\u001b[1;32m    588\u001b[0m \u001b[39mArgs:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    592\u001b[0m \u001b[39m    a collections of iterators\u001b[39;00m\n\u001b[1;32m    593\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    594\u001b[0m \u001b[39m# dataloaders are Iterable but not Sequences. Need this to specifically exclude sequences\u001b[39;00m\n\u001b[0;32m--> 595\u001b[0m \u001b[39mreturn\u001b[39;00m apply_to_collection(loaders, Iterable, \u001b[39miter\u001b[39;49m, wrong_dtype\u001b[39m=\u001b[39;49m(Sequence, Mapping))\n",
      "File \u001b[0;32m~/miniconda3/envs/pytorch-lightning-env/lib/python3.9/site-packages/lightning_utilities/core/apply_func.py:47\u001b[0m, in \u001b[0;36mapply_to_collection\u001b[0;34m(data, dtype, function, wrong_dtype, include_none, *args, **kwargs)\u001b[0m\n\u001b[1;32m     45\u001b[0m \u001b[39m# Breaking condition\u001b[39;00m\n\u001b[1;32m     46\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(data, dtype) \u001b[39mand\u001b[39;00m (wrong_dtype \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mor\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39misinstance\u001b[39m(data, wrong_dtype)):\n\u001b[0;32m---> 47\u001b[0m     \u001b[39mreturn\u001b[39;00m function(data, \u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m     49\u001b[0m elem_type \u001b[39m=\u001b[39m \u001b[39mtype\u001b[39m(data)\n\u001b[1;32m     51\u001b[0m \u001b[39m# Recursively apply to collection items\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/pytorch-lightning-env/lib/python3.9/site-packages/torch/utils/data/dataloader.py:435\u001b[0m, in \u001b[0;36mDataLoader.__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    433\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_iterator\n\u001b[1;32m    434\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 435\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_get_iterator()\n",
      "File \u001b[0;32m~/miniconda3/envs/pytorch-lightning-env/lib/python3.9/site-packages/torch/utils/data/dataloader.py:381\u001b[0m, in \u001b[0;36mDataLoader._get_iterator\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    379\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    380\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcheck_worker_number_rationality()\n\u001b[0;32m--> 381\u001b[0m     \u001b[39mreturn\u001b[39;00m _MultiProcessingDataLoaderIter(\u001b[39mself\u001b[39;49m)\n",
      "File \u001b[0;32m~/miniconda3/envs/pytorch-lightning-env/lib/python3.9/site-packages/torch/utils/data/dataloader.py:1072\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter.__init__\u001b[0;34m(self, loader)\u001b[0m\n\u001b[1;32m   1070\u001b[0m _utils\u001b[39m.\u001b[39msignal_handling\u001b[39m.\u001b[39m_set_SIGCHLD_handler()\n\u001b[1;32m   1071\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_worker_pids_set \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n\u001b[0;32m-> 1072\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_reset(loader, first_iter\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n",
      "File \u001b[0;32m~/miniconda3/envs/pytorch-lightning-env/lib/python3.9/site-packages/torch/utils/data/dataloader.py:1105\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter._reset\u001b[0;34m(self, loader, first_iter)\u001b[0m\n\u001b[1;32m   1103\u001b[0m \u001b[39m# prime the prefetch loop\u001b[39;00m\n\u001b[1;32m   1104\u001b[0m \u001b[39mfor\u001b[39;00m _ \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_prefetch_factor \u001b[39m*\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_workers):\n\u001b[0;32m-> 1105\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_try_put_index()\n",
      "File \u001b[0;32m~/miniconda3/envs/pytorch-lightning-env/lib/python3.9/site-packages/torch/utils/data/dataloader.py:1339\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter._try_put_index\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1336\u001b[0m \u001b[39massert\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_tasks_outstanding \u001b[39m<\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_prefetch_factor \u001b[39m*\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_workers\n\u001b[1;32m   1338\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m-> 1339\u001b[0m     index \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_next_index()\n\u001b[1;32m   1340\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mStopIteration\u001b[39;00m:\n\u001b[1;32m   1341\u001b[0m     \u001b[39mreturn\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/pytorch-lightning-env/lib/python3.9/site-packages/torch/utils/data/dataloader.py:618\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter._next_index\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    617\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_next_index\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[0;32m--> 618\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mnext\u001b[39;49m(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_sampler_iter)\n",
      "Cell \u001b[0;32mIn[20], line 25\u001b[0m, in \u001b[0;36mTaskBatchSampler.__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__iter__\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[1;32m     23\u001b[0m     \u001b[39m# Aggregate multiple batches before returning the indices\u001b[39;00m\n\u001b[1;32m     24\u001b[0m     batch_list \u001b[39m=\u001b[39m []\n\u001b[0;32m---> 25\u001b[0m     \u001b[39mfor\u001b[39;00m batch_idx, batch \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbatch_sampler):\n\u001b[1;32m     26\u001b[0m         batch_list\u001b[39m.\u001b[39mextend(batch)\n\u001b[1;32m     27\u001b[0m         \u001b[39mif\u001b[39;00m (batch_idx\u001b[39m+\u001b[39m\u001b[39m1\u001b[39m) \u001b[39m%\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtask_batch_size \u001b[39m==\u001b[39m \u001b[39m0\u001b[39m:\n",
      "Cell \u001b[0;32mIn[10], line 70\u001b[0m, in \u001b[0;36mFewShotBatchSampler.__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     68\u001b[0m index_batch \u001b[39m=\u001b[39m []\n\u001b[1;32m     69\u001b[0m \u001b[39mfor\u001b[39;00m c \u001b[39min\u001b[39;00m class_batch:  \u001b[39m# For each class, select the next K examples and add them to the batch\u001b[39;00m\n\u001b[0;32m---> 70\u001b[0m     index_batch\u001b[39m.\u001b[39;49mextend(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mindices_per_class[c][start_index[c]:start_index[c]\u001b[39m+\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mK_shot])\n\u001b[1;32m     71\u001b[0m     start_index[c] \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mK_shot\n\u001b[1;32m     72\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39minclude_query:  \u001b[39m# If we return support+query set, sort them so that they are easy to split\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/pytorch-lightning-env/lib/python3.9/site-packages/torch/_tensor.py:926\u001b[0m, in \u001b[0;36mTensor.__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    917\u001b[0m \u001b[39mif\u001b[39;00m torch\u001b[39m.\u001b[39m_C\u001b[39m.\u001b[39m_get_tracing_state():\n\u001b[1;32m    918\u001b[0m     warnings\u001b[39m.\u001b[39mwarn(\n\u001b[1;32m    919\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mIterating over a tensor might cause the trace to be incorrect. \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    920\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mPassing a tensor of different shape won\u001b[39m\u001b[39m'\u001b[39m\u001b[39mt change the number of \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    924\u001b[0m         stacklevel\u001b[39m=\u001b[39m\u001b[39m2\u001b[39m,\n\u001b[1;32m    925\u001b[0m     )\n\u001b[0;32m--> 926\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39miter\u001b[39m(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49munbind(\u001b[39m0\u001b[39;49m))\n",
      "\u001b[0;31mRuntimeError\u001b[0m: setStorage: sizes [], strides [], storage offset 13, and itemsize 8 requiring a storage size of 112 are out of bounds for storage of size 0"
     ]
    }
   ],
   "source": [
    "EEG_INPUT_DIM = 310\n",
    "EYE_INPUT_DIM = 33\n",
    "OUTPUT_DIM = 12\n",
    "LAYER_SIZES = [np.random.randint(100,200), np.random.randint(20,50), OUTPUT_DIM]\n",
    "NUM_EMOTIONS = N_WAY\n",
    "\n",
    "protomaml_model = train_model(ProtoMAML, \n",
    "                              lr=1e-3, \n",
    "                              lr_inner=0.1,\n",
    "                              lr_output=0.1,\n",
    "                              num_inner_steps=1,\n",
    "                              model_args = (EEG_INPUT_DIM, EYE_INPUT_DIM, LAYER_SIZES,\n",
    "                                LAYER_SIZES, OUTPUT_DIM, NUM_EMOTIONS, device),\n",
    "                              train_loader=train_protomaml_loader, \n",
    "                              val_loader=val_protomaml_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.float32\n",
      "torch.int64\n",
      "torch.float32\n",
      "torch.int64\n"
     ]
    }
   ],
   "source": [
    "train_features_batch, train_labels_batch = next(iter(train_protomaml_loader))[1]\n",
    "print(train_features_batch[0].dtype)\n",
    "print(train_labels_batch[0].dtype)\n",
    "val_features_batch, val_labels_batch = next(iter(val_protomaml_loader))[0]\n",
    "print(val_features_batch[0].dtype)\n",
    "print(val_labels_batch[0].dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Opens tensorboard in notebook. Adjust the path to your CHECKPOINT_PATH if needed\n",
    "%tensorboard --logdir ../saved_models/seed-v/tensorboards/ProtoMAML/"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test meta learning model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# oh, i think you just need to convert train_loader to a lightning thingy\n",
    "    #error ->>> TypeError(f\"`Trainer.fit()` requires a `LightningModule`, got: {model.__class__.__qualname__}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch-lightning-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "37b693a6f780c8511f891a639fe901e9eb59e5ec0a37b86f170d6f7c2ce5e026"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
